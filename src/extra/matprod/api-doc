MATPROD - A LIBRARY FOR MATRIX MULTIPLICATION
          Documentation for Application Program Interface to Library Procedures

Copyright (C) 2013, 2014, 2017, 2018 Radford M. Neal.

  The matprod library is free software; you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation; either version 2 of the License, or
  (at your option) any later version.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License along
  with this program; if not, write to the Free Software Foundation, Inc.,
  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.


The matprod library provides double-precision real matrix/vector by
matrix/vector multiplication routines.  The motivating application is
implementation of matrix multiplication operators in a language such
as R, but the routines could also be used directly in an application
program.  Distinguishing features of this library are

   - Exact reproducibility between platforms, with the results being
     identical to what would be produced with a naive implementation.

   - No allocation of temporary storage (except, of course, for small
     amounts on the procedure-call stack, with only matprod_trans12
     allocating a significant amount, up to about 16K bytes).

   - Good performance, subject to the above two requirements.

   - Optional parallel implementation using the "helpers" library.

The routines should work with any C compiler implementing the C99
standard, and for any processor implementing IEEE standard
floating-point arithmetic.  However, some performance improvements are
enabled only for a gcc look-alike compiler, and SIMD instructions are
used only for Intel/AMD processors.  Performance tuning has been done
using gcc 7.2, and may not be optimal for other compilers.  Testing
has been primarily done on Intel/AMD machines, of a variety of
vintages.

Note that matrices are assumed to be stored in FORTRAN memory order,
proceeding down columns (not across rows).  To use the matprod
routines with matrices stored in standard C (row-major) order, just
reverse the order of the input operands (and exchange matprod_trans1
and matprod_trans2, as well as matprod_vec_mat and matprod_mat_vec).


Reproducibility.

These routines have been written to produce exactly the same results
as the obvious matrix multiplication algorithms, which sequentially
sum products of elements.  They can hence replace such naive
implementations without changing the results.  The results will also
be reproducible on different machines, provided all machines implement
IEEE standard double-precision floating point operations.  In
particular, these routines do not bypass multiplications by zero, and
therefore propagate Inf and NaN values properly.  They do not re-group
additions assuming associativity holds, so round-off errors and
overflows will be the same as for the obvious algorithms.

Note that producing the exact same result as the obvious algorithms
requires that the routines be built disabling any contraction of
floating-point operations (eg, fused multiply-add) that might be done
by the compiler/processor, and that all operations must be done with
64-bit floating-point numbers, without any extra precision for
intermediate results.

An unfortunate design choice in Intel/AMD processors will introduce
one source of non-reproducibility on platforms using them - when both
operands of a multiplication or addition are NaN values, one or the
other NaN is propagated, but it will be arbitrary which, because Intel
failed to ensure that addition and multiplication are commutative in
this situation, and C code has no control over the order of operands
in instructions generated by the compiler.


Parallel operation.

The matprod routines can be used via task procedures that can run in
"helper" threads, as supported by the helpers library (available at
https://github.com/radfordneal/helpers).

One option is to use a single helper thread for each matrix
multiplication operation, allowing this operation to execute in
parallel with other operations done by the application (including
other matrix multiplication operations).  In many situations, the
output of one such operation can be pipledlined as the input for
another operation executing concurrently.

It is also possible to use more than one helper thread for a single
multiplication operation, perhaps while also pipelining its input or
output.


Performance.

These routines are written with a fair amount of attention to cache
performance and other efficiency issues.  Optional code is provided
that takes advantage of SSE2, SSE3, AVX, or AVX2 instructions on
Intel/AMD processors.  The routines can also optionally be built with
assumptions about alignment of arguments that may allow improved
performance.

Nevertheless, these routines may not be as fast as the matrix
multiplication routines in optimized BLAS libraries.  A general design
goal is for the non-parallel version of the routines to be no more
than a factor of two slower than the best non-parallel implementation
of matrix multiply (without the constraints on reproducibility).  This
goal is met for the most part.  The speed-up from parallelization is
also comparable.  For some operands, the matprod routines are actually
faster than the routines provided in some optimized BLAS libraries, in
some cases substantially faster - primarily when at least one operand
has at least one small dimension.

See imp-doc and the comments in matprod.c and par-matprod.c for
details on the algorithms used.


Building and using the procedures.

The non-parallel routines, in matprod.c, are called in ordinary C
fashion.  The parallel, pipelined routines, in par-matprod.c, take the
form of task procedures for use with the helpers library for parallel
computation (see https://github.com/radfordneal/helpers), though
convenient procedures for scheduling these task procedures are also
provided.

The matprod.c and/or par-matprod.c source files should be compiled and
linked with the program that uses the routines in the usual fashion.
Some options can be set by pre-processor symbols defined with
arguments to the compiler, or alternatively, by defining the symbol
MATPROD_APP_INCLUDED (as anything), which will cause matprod.c and
par-matprod.c to include the file matprod-app.h, which may define
symbols that set these options.

These options include the following:

    ALIGN                If defined (must be a power of 2), the matprod
                         and par_matprod functions will assume that
                         their arguments are aligned to such a
                         boundary (plus ALIGN_OFFSET, if defined).
                         This assumption may allow for faster and
                         smaller code.

    ALIGN_OFFSET         If ALIGN is defined, ALIGN_OFFSET gives the
                         offset from an even multiple of ALIGN that
                         arguments are aligned to.  It must be a
                         multiple of eight.  If ALIGN_OFFSET is not
                         defined, zero is assumed.

    NO_ASSUME_ALIGNED    If defined (as anything), the gcc builtin
                         function __builtin_assume_aligned, which
                         informs the compiler of alignment, will not
                         be used.  The matprod code may still use the
                         settings of ALIGN and ALIGN_OFFSET to
                         explicitly handle alignment.  This is meant
                         for performance testing, or for old gcc
                         versions without __builtin_assume_aligned.
                         It is not needed with a compiler that isn't
                         a gcc look-alike.

    MATPROD_NO_RESTRICT  If defined, the arguments of the matprod and
                         par_matprod functions will not be declared
                         with the "restricted" keyword.  This is meant
                         for performance testing - results are not
                         guaranteed correct in any case if the output
                         argument aliases with an input.

Some special code has been written to take advantage of the SSE2,
SSE3, AVX, AVX2 instructions available on some Intel/AMD processors.
It is used only if symbols __SSE2__, __SSE3__, etc. are pre-defined,
indicating that the builtin intrinsic functions that access the
corresponding instructons are available.  This SIMD code can be
disabled by defining the symbol DISABLE_SIMD_CODE.  Just the AVX or
AVX2 code can be disabled by defining DISABLE_AVX_CODE (in which case
SSE2 or SSE3 code may still be used).  Some AVX or AVX2 code appears
to not be better than the corresponding SSE2, SSE3, or non-SIMD code;
it is disabled unless the symbol ENABLE_ALL_AVX_CODE is defined (and
DISABLE_AVX_CODE is not defined).  The SIMD code sections are
automatically disabled when the routines are compiled for an
architecture without the required instructions.

Note that disabling the explicit SIMD code does not prevent the
compiler from generating SIMD instructions, if this is allowed by the
compiler options used.  Also note that depending on what architecture
the compiler is told to generate code for, intrinsics designed for
SSE2 may actually generate AVX instructions (and perhaps similarly for
SSE2/SSE3, SSE3/SSE4.1, AVX/AVX2, etc.).  However, code enabled for
only SSE2, or SSE3, or AVX should work on a processor having only the
specified level of SIMD instructions, if compiled for that processor.


Matrix operations using procedures in matprod.c.

The procedures below are in matprod.c, with declarations in matprod.h.
Their prototypes below are written with pointers declared as simply
"double *", but, the actual declaration will be "double * restrict"
unless the matprod source is compiled with -DMATPROD_NO_RESTRICT.
Note that the dimensions of the matrices (numbers or rows and columns)
have C int type, but the total number of elements in a matrix may be
larger than can be stored in an int.

    void matprod_scalar_vec (double x, double *y, double *z, int m)

      Multiply scalar x and vector y, of dimension m, storing the
      result in vector z, of dimension m.

    double matprod_vec_vec (double *x, double *y, int k)

      Multiply row vector x and column vector y, both of dimension k,
      returning the result.

    void matprod_vec_mat (double *x, double *y, double *z, int k, int m)

      Multiply row vector x, of dimension k, and matrix y, of dimension
      k x m, storing the result in row vector z, of dimension m.

    void matprod_mat_vec (double *x, double *y, double *z, int n, int k)

      Multiply matrix x, of dimension n x k, and column vector y, of
      dimension k, storing the result in column vector z, of dimension n.

    void matprod_outer (double *x, double *y, double *z, int n, int m)

      Multiply the length-n column vector x (ie, n x 1 matrix) by the
      length-m row vector y (ie, 1 x m matrix), storing the result in
      the matrix z, of dimension n x m.

    void matprod_mat_mat (double *x, double *y, double *z, int n, int k, int m)

      Multiply matrix x, of dimension n x k, and matrix y, of
      dimension k x m, storing the result in matrix z, of dimension 
      n x m.

    void matprod_trans1 (double *x, double *y, double *z, int n, int k, int m)

      Multiply the transpose of matrix x, of dimension k x n, and
      matrix y, of dimension k x m, storing the result in matrix z, of
      dimension n x m.  If x and y are the same matrix, so the result
      is symetrical, redundant computations of symetric elements are
      avoided.

    void matprod_trans2 (double *x, double *y, double *z, int n, int k, int m)

      Multiply matrix x, of dimension n x k, and the transpose of
      matrix y, of dimension m x k, storing the result in matrix z, of
      dimension n x m.  If x and y are the same matrix, so the result
      is symetrical, redundant computations of symetric elements are
      avoided.

    void matprod_trans12 (double *x, double *y, double *z, int n, int k, int m)

      Multiply the transpose of matrix x, of dimension k x n, and the
      transpose of matrix y, of dimension m x k, storing the result in
      matrix z, of dimension n x m.

    void matprod_fill_lower (double *z, int n)

      Fills the lower triangle of the n x n matrix z with the symetric
      elements from the upper triangle.  This is useful when computing
      a symmetrical matrix product using the BLAS routine DSYRK.

For the above procedures, the result matrix, z, must have no overlap
with the operands, x and y.  However, the operands may be the same or
may overlap.

Some attention has been paid to making special cases of these routines
be fast.  Some cases where n, k, or m is 2, 3, or 4 are handled with
special code, when the general procedure would be significantly
slower.  Also, matprod_vec_mat and other procedures will simply call
matprod_scalar_vec when the operation reduces to multiplication by a
scalar.  Similarly,the matprod_outer procedure will be called from
matprod_mat_mat, matprod_trans1, matprod_trans2, and matprod_trans12
when the value of k passed is one.  Also, cases of matprod_mat_mat,
matprod_trans1, matprod_trans2, and matprod_trans12 that reduce to
matprod_vec_mat, or matprod_mat_vec will call the specialized routine,
and similarly cases of matprod_mat_vec and matprod_vec_mat that reduce
to matprod_vec_vec will call that routine.  Calling a specialized
routine directly will of course be slightly faster when it is known
that it is appropriate.


Parallel operation using procedures in par-matprod.c.

Task procedures are defined in par-matprod.c (and declared in
par-matprod.h) for use with the "helpers" facility.  An operation may
be scheduled as a single task, potentially allowing it to run
concurrently with the master thread or other task procedures, perhaps
while pipelining data in or out.  Alternatively, several tasks may be
scheduled for a single operation, with each task computing a portion
of the result (again, perhaps with pipelining of data to/from other
tasks), thereby allowing parallel computation for a single operation.

Pipelining of the input may be done for the second operand for all
operations except matprod_trans2 and matprod_trans12.  Pipelining of
the output may be done for vector-matrix multiplies and matrix-matrix
multiplies except for matprod_trans12.  Note that, to reduce overhead,
columns or elements of pipelined output may not always be made
available immediately after being computed.

Using these routines requires the helpers.h and helpers.c files from
the helpers library, available at github.com/radfordneal/helpers.  A
helpers-app.h file must also be provided, which must contain the type
definitions required for the helpers routines, including a "pointer"
type for operands (which may or may not be an actual pointer).  The
helpers-app.h file must also define a LENGTH macro that returns the
number of elements in an operand or result matrix (equal to the number
of rows times the number of columns), and a REAL macro that returns a
pointer to the double-precision data for an operand or result matrix
(or these macros can instead be defined in a matprod-app.h file, which
is included if MATPROD_APP_INCLUDED is defined).

Note that the application will need to use the facilities provided by
the "helpers" package (eg, helpers_wait_until_not_being_computed) to
determine when the result of an operation is available.

The task procedures (described below) can be scheduled to be run by
the application.  More typically, however, the par_matprod procedures
described below (also defined in par-matprod.c and declared in
par-matprod.h) will be used to schedule the appropriate tasks.


The par_matprod procedures.

The operands for the par_matprod procedures are of type helpers_var_t
(not direct pointers to the matrices).  The procedures all take a
"split" argument, which is the desired number of tasks to split the
operation into.  The actual number of tasks used might be less than
"split", if it seems that this will give better performance.  A
negative value can be used for "split", in which case the negative of
"split" will be used, with this value reduced only if that degree of
splitting is not possible (given implementation constraints).  A value
of zero for "split" indicates that the corresponding matprod procedure
should be called immediately without scheduling a task, after waiting
for the operands to be computed if they have not been already.

If the final 'pipe' argument is non-zero, the tasks scheduled by the
par_matprod procedures use pipelining for the second operand and the
output, if this is possible.  This argument is present (but ignored)
even for procedures for which pipelining is currently never possible.

Note that it has deliberately been left to the application to choose
whether to do an operation immediately (setting 'split' to zero) or to
instead schedule a task or tasks to do it (with the number of tasks
limited by 'split', but possibly reduced according to heuristics used
by the par_matprod procedure).  That is, if 'split' is non-zero, the
par_matprod procedure will always schedule a task or tasks, regardless
of how small the operation is (even if it is so small that the time
required to schedule the task is greater than the time to just do it).
This is because the best decision may depend on factors such as
whether the master thread has any other work to do currently, and
whether the operands of the operation have not yet been computed, so
trying to do it immediately would require waiting.

Note also that the par_matprod procedures do not check for special
cases (for example, par_matprod_mat_mat does not check whether the
operation is actually a dot product doable with par_matprod_vec_vec).
These special cases will instead be detected only after the task
procedure has started.  Accordingly, some non-negligible overhead can
be avoided if the appropriate par_matprod procedure is called
originally.  Also, the decisions taken by the par_matprod procedures
regarding splitting into multiple tasks may not be optimal for such
special cases.

The following par_matprod procedures are available:

    void par_matprod_scalar_vec (helpers_var_ptr z, helpers_var_ptr x, 
                                 helpers_var_ptr y, int split, int pipe)

      Schedules a task to perform a scalar-vector dot product, of the
      scalar stored in x and the vector stored in y, with possible
      pipelining of the input y and of the output.

    void par_matprod_vec_vec (helpers_var_ptr z, helpers_var_ptr x, 
                              helpers_var_ptr y, int split, int pipe)

      Schedules a task to perform a vector-vector dot product, with
      possible pipelining of input y.  The result is stored in the
      scalar operand z.  At present, this operation is always done in
      only one thread, so 'split' is relevant only in so far as it is
      zero or non-zero.

    void par_matprod_vec_mat (helpers_var_ptr z, helpers_var_ptr x, 
                              helpers_var_ptr y, int split, int pipe)

      Schedules a task to perform a vector-matrix product, with
      possible pipelining of input y and of the output.

    void par_matprod_mat_vec (helpers_var_ptr z, helpers_var_ptr x, 
                              helpers_var_ptr y, int split, int pipe)

      Schedules a task to perform a matrix-vector product, with
      possible pipelining of input y, but not of the output.

    void par_matprod_outer (helpers_var_ptr z, helpers_var_ptr x, 
                            helpers_var_ptr y, int split, int pipe)

      Schedules a task to perform a vector outer product, with
      possible pipelining of input y and of the output.

    void par_matprod_mat_mat (helpers_var_ptr z, helpers_var_ptr x, 
                              helpers_var_ptr y, int k, int split, int pipe)

      Schedules a task to perform a matrix-matrix product, with k
      being the number of columns in x and rows in y, with possible
      pipelining of input y and of the output.

    void par_matprod_trans1 (helpers_var_ptr z, helpers_var_ptr x, 
                             helpers_var_ptr y, int k, int split, int pipe)

      Schedules a task to perform a matrix-matrix product with first
      operand transposed, with k being the number of rows in x and y,
      with possible pipelining of input y and of the output.

    void par_matprod_trans2 (helpers_var_ptr z, helpers_var_ptr x, 
                             helpers_var_ptr y, int k, int split, int pipe)

      Schedules a task to perform a matrix-matrix product with second
      operand transposed, with k being the number of columns in x and
      y, with possible pipelining of the output, but not of the input.

    void par_matprod_trans12 (helpers_var_ptr z, helpers_var_ptr x, 
                              helpers_var_ptr y, int k, int split, int pipe)

      Schedules a task to perform a matrix-matrix product with both
      operands transposed, with k being the number of rows in x and
      columns in y, with no pipelining of either input or output.


Task procedures.

An application can instead do its own scheduling of task procedures,
though this should generally be unnecessary.  These task procedures
are declared in par-matprod.h to be of type helpers_task_proc, for
which the arguments are an op value, a pointer to the result matrix
(which is possibly a vector or scalar), a pointer to the first operand
matrix/vector, and a pointer to the second operand matrix/vector.

When an operation is to be done by a single task, the op value passed
to the task procedure should be zero except for the mat_mat, trans1,
trans2, and trans12 procedures, for which it should be the value of k
(the number of products summed to compute each element of the result).

When an operation is to be split between several tasks, the low 32
bits of the op value should be the value of k (or zero, for tasks not
requiring k), the 8 bits above that should be the number of tasks used
for the operation minus one, and the 8 bits above that should be the
portion (numbered from zero) that this task does.  The tasks should be
scheduled scheduled sequentially starting with the one doing portion
zero, with pipelining of each task's output to the next task's output
operand.  They may also be scheduled with pipelining of the second
input (but this has no effect for matprod_trans2 and matprod_trans12).
The task doing the final portion may if desired be scheduled with
output pipelining, though this is potentially inefficient for the
vec_vec, mat_vec, and trans12 operations, since they do not do any
output pipelining.

The following task procedures (found in par-matprod.c) are provided,
corresponding to the direct matprod procedures above (except for
matprod_fill_lower):

    task_par_matprod_scalar_vec

      Scalar-vector multiply, with element-by-element pipelining of
      the second operand and of the output.

    task_par_matprod_vec_vec

      Vector-vector multiply, with element-by-element pipelining of
      the second operand.  At present, this operation is always done
      in only one thread, even if more than one task is scheduled.

    task_par_matprod_vec_mat

      Vector-matrix multiply, with column-by-column pipelining of the
      second operand, and element-by-element pipelining of the output.

    task_par_matprod_mat_vec

      Matrix-vector multiply, with element-by-element pipelining of
      the second operand.

    task_par_matprod_outer

      Matrix outer product of column vector and row vector, with
      element-by-element pipelining of the second operand, and
      column-by-column pipelining of the output.

    task_par_matprod_mat_mat

      Matrix-matrix multiply, with column-by-column pipelining of the
      second operand, and column-by-column pipelining of the output.

    task_par_matprod_trans1

      Multiplies the transpose of first operand matrix and the second
      operand matrix, with column-by-column pipelining of the second
      operand, and column-by-column pipelining of the output.

    task_par_matprod_trans2

      Multiplies the first operand matrix and the transpose of the
      second operand matrix, with column-by-column pipelining of the
      output, but no pipelining of the operands.  (However, this task
      procedure does explicitly wait for the second operand to become
      available, in case it has been scheduled with pipelining of this
      operand, since it is pipelined for many of the other task
      procedures.)

    task_par_matprod_trans12

      Multiplies the transpose of the first operand matrix and the
      transpose of the second operand matrix, with no pipelining of
      the output and no pipelining of the operands.  (However, this
      task procedure does explicitly wait for the second operand to
      become available, in case it has been scheduled with pipelining
      of this operand, since it is pipelined for many of the other
      task procedures.)
